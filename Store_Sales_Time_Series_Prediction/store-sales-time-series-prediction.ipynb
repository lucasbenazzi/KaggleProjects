{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lucasbenazzicestari/store-sales-time-series-prediction?scriptVersionId=94276839\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"**Disclaimer** - This notebook will focus only on making predictions for the Store Sales - Time Series Forecasting competition; it won't do any in-depth analysis as there are already plenty of well-made resources available for that.\n\n# **Introduction**\nThe predictions will be made using the already available datasets from the competition. The sales information is divided into product families, so each day there's information on how much a product family is sold as well as how many were on promotion. There is also oil price information available, which will be useful as Ecuador is an oil-dependent country. And finally, there's information on all holidays and relevant events for the country.\n\n# **Objective**\nThe goal of this notebook will be to create multiple models for each store, average their predictions, and make a competition submission. In order to do this, we will preprocess the data and create a Store class to make individual predictions.\n\n# **Relevant datasets**\n**Oil** - This dataset consists only of the oil prices for each given day. This information needs some preprocessing as there are some missing values. In addition to this, two columns will be added, one with the moving average and another with the moving standard deviation.\n\n**Holidays Events** - This dataset has a description of all events and holidays and which regions of the country were affected. Since we have the region for each store, we can see which holidays and events affected which stores. The dataset still needs to have transformations done in order to get a list of affected stores. Alongside of that, there are many different types of holidays and events that happened during this period, so there will be many simplifications. The goal will be to determine, for each store, if a given day had an event or holiday, and classify that date as \"Non working day\".\n\n**Stores** - For each store, there's information on product families being sold and being on promotion. Since the goal is to predict sales, product family sales will be used as the label and promotion information will be used as input for training the model.\n\n# **Prediction**\nTwo models will be trained: XGBoost and Random Forest regressor. The final prediction will be the average of the predictions of each model.","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\n\nfrom calendar import monthrange\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import MultiOutputRegressor","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:26:11.864239Z","iopub.execute_input":"2022-04-28T07:26:11.864752Z","iopub.status.idle":"2022-04-28T07:26:13.047265Z","shell.execute_reply.started":"2022-04-28T07:26:11.864713Z","shell.execute_reply":"2022-04-28T07:26:13.046533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"df_holidays_events = pd.read_csv(\"../input/store-sales-time-series-forecasting/holidays_events.csv\",\n                                 parse_dates = ['date'])\n\ndf_oil = pd.read_csv(\"../input/store-sales-time-series-forecasting/oil.csv\",\n                     parse_dates = ['date'])\n\ndf_stores = pd.read_csv(\"../input/store-sales-time-series-forecasting/stores.csv\")\n\ndf_test = pd.read_csv(\"../input/store-sales-time-series-forecasting/test.csv\",\n                      parse_dates = ['date'])\n\ndf_train = pd.read_csv(\"../input/store-sales-time-series-forecasting/train.csv\",\n                      parse_dates = ['date'])\n\ndf_sample_submission = pd.read_csv(\"../input/store-sales-time-series-forecasting/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:26:13.048885Z","iopub.execute_input":"2022-04-28T07:26:13.049153Z","iopub.status.idle":"2022-04-28T07:26:16.17533Z","shell.execute_reply.started":"2022-04-28T07:26:13.049115Z","shell.execute_reply":"2022-04-28T07:26:16.174609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Oil data","metadata":{}},{"cell_type":"code","source":"# importing oil data and adds two columns\noil = df_oil.copy()\noil = oil.set_index('date')\noil = oil['dcoilwtico'].resample('D').sum().reset_index()\noil = oil.replace({0:np.nan})\noil['dcoilwtico'] = oil['dcoilwtico'].interpolate(limit_direction = 'both')\noil['dcoilwtico mean'] = oil['dcoilwtico'].rolling(7).mean().interpolate(limit_direction = 'both')\noil['dcoilwtico std'] = oil['dcoilwtico'].rolling(7).std().interpolate(limit_direction = 'both')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:26:16.177588Z","iopub.execute_input":"2022-04-28T07:26:16.178224Z","iopub.status.idle":"2022-04-28T07:26:16.203449Z","shell.execute_reply.started":"2022-04-28T07:26:16.178185Z","shell.execute_reply":"2022-04-28T07:26:16.202843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Holiday and Events data","metadata":{}},{"cell_type":"code","source":"# defining which stores were affected by which event or holiday\nholidays_events = df_holidays_events.copy()\n\n# the next couple of changes were made analysing the data. There was an official transfer for new years eve holiday, but that didn't translate on a difference in sales.\nholidays_events.loc[297, 'transferred'] = False\nholidays_events = holidays_events.loc[~(holidays_events.index == 298)]\n\nholidays_events = holidays_events.loc[holidays_events['transferred'] == False].drop('transferred', axis = 1)\nholidays_events = holidays_events.loc[holidays_events['type'] != \"Work Day\"]\n\nstores = df_stores.copy()\n\ndef affected_stores(holiday_locale, holiday_locale_name):\n    if holiday_locale == 'National':\n        return stores['store_nbr'].unique()\n    \n    elif holiday_locale == 'Local':\n        return stores['store_nbr'].loc[stores['city'] == holiday_locale_name].to_numpy()\n    elif holiday_locale == 'Regional':\n        return stores['store_nbr'].loc[stores['state'] == holiday_locale_name].to_numpy()\n    else:\n        return []\n\nholidays_events['cities'] = holidays_events.apply(lambda x : affected_stores(x['locale'],x['locale_name']), axis = 1)\n\nholidays_events = holidays_events.drop(columns = ['type','locale','locale_name','description'], axis = 0)\n\ncities_dummies = cities_dummies = pd.get_dummies(holidays_events['cities'].explode()).sum(level=0)\nholidays_with_dummies = pd.concat([holidays_events, cities_dummies], axis = 1).drop(['cities'], axis = 1)\n\nholidays_events = pd.melt(holidays_with_dummies, id_vars = 'date', var_name = 'store_nbr').drop(['value'], axis = 1)\nholidays_by_store = holidays_events.groupby(['date','store_nbr']).sum().reset_index() ","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:26:16.205308Z","iopub.execute_input":"2022-04-28T07:26:16.205687Z","iopub.status.idle":"2022-04-28T07:26:16.318013Z","shell.execute_reply.started":"2022-04-28T07:26:16.205653Z","shell.execute_reply":"2022-04-28T07:26:16.317325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions class","metadata":{}},{"cell_type":"code","source":"class AuxiliaryFunctions():\n    \"\"\"Class with auxiliary functions to be used in the\n    Store class.\n    \n    \"\"\"\n    def get_first_day_sold(self, store_nbr):\n        \"\"\"Gets the day the given store had its first\n        sale.\n        \n        Args:\n            store_nbr (int): Unique id of the store.\n        Returns:\n            Datetime of the first sale.\n        \n        \"\"\"\n        \n        df = df_train.copy()\n        df = df.loc[df['store_nbr'] == store_nbr, ['date','sales']]\n        df = df.groupby('date').sum()\n        return df.ne(0).idxmax().values[0] + np.timedelta64(1,'D')\n\n    def prepare_test_inputs(self, store_nbr, start_date, end_date):\n        \"\"\"Gets the input and label values from the train\n        dataframe with their relevant values unstacked. It\n        takes two dates as input to use as a date range.\n        \n        X input has the family column unstacked with the\n        promotion values.\n        y input has the family column unstacked with the\n        sales values.\n        \n        Args:\n            start_date (Datetime): Day from which to start.\n            end_date (Datetime): Day from which to end.\n            \n        Returns:\n            The inputs (X) and labels (y) for training.\n        \"\"\"\n        \n        df = df_train.copy()\n\n        df = df.loc[(df['date'] >= start_date) & (df['date'] <= end_date)]\n        df = df.set_index(['family','date']).sort_index().drop('id', axis = 1)\n        \n        X = df.loc[df['store_nbr'] == store_nbr].drop(['store_nbr','sales'], axis = 1).unstack('family')\n        X.columns = [name for _, name in X.columns]\n        \n        y = df.loc[df['store_nbr'] == self.store_nbr].drop(['store_nbr','onpromotion'], axis = 1).unstack('family')\n        y.columns = [name for _, name in y.columns]\n        \n        X = X.reset_index()\n        return X, y      \n    \n    def add_information(self, dataframe):\n        \"\"\"Adds relevant data not directly present in the\n        train dataframe.\n\n        It adds data related to oil prices, time information,\n        and which dates to consider as work days.\n\n        Args:\n            dataframe (DataFrame): Base dataframe which will\n                have the information added.\n\n        Returns:\n            A dataframe with its base values and extra\n            information combined.\n\n        \"\"\"\n\n        X = dataframe.copy()\n\n        X = X.merge(oil, on = ['date'], how = 'left')\n\n        timestamp_s = X['date'].map(pd.Timestamp.timestamp)\n\n        day = 24 * 60 * 60\n        week = day * 7\n        year = 365.2425 * day\n        quarter = year / 4\n        half_decade = year * 5\n\n        X['week sin'] = np.sin(timestamp_s * (2 * np.pi / week))\n        X['week cos'] = np.cos(timestamp_s * (2 * np.pi / week))\n\n        X['quarter sin'] = np.sin(timestamp_s * (2 * np.pi / quarter))\n        X['quarter cos'] = np.cos(timestamp_s * (2 * np.pi / quarter))\n\n        X['year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n        X['year cos'] = np.cos(timestamp_s * (2 * np.pi / year))    \n\n        X['half decade sin'] = np.sin(timestamp_s * (2 * np.pi / half_decade))\n        X['half decade cos'] = np.cos(timestamp_s * (2 * np.pi / half_decade))    \n\n        X['day of week'] = X['date'].dt.dayofweek\n\n        X['is new year'] = 0\n        X.loc[X['date'].dt.dayofyear == 1, 'is new year'] = 1\n\n        X['is work day'] = 1\n        X['quarter'] = X['date'].dt.month // 4\n\n        store_holidays = holidays_by_store.loc[holidays_by_store['store_nbr'] == self.store_nbr, 'date']\n        X.loc[\n            (X['date'].isin(store_holidays)) | (X['day of week'] > 4) |\n            (X['date'].dt.day == 15)\n            ,\n            'is work day'] = 0\n\n        X['is payday'] = X['date'].map(\n            lambda date: 1 if (date.day == monthrange(date.year, date.month)[1] or date.day == 15) else 0\n        )\n        X = pd.get_dummies(X, columns = ['day of week'])\n\n        return X.set_index('date')\n\n    def train_test_split(self, store_nbr, number_of_days = 15):\n        \"\"\"Creates a train test split based on a given number\n        of days, which specifies the number of days used for\n        testing. The split isn't randomized and the test data\n        is always after the training data.\n        \n        Args:\n            store_nbr (int): Unique id of the store.\n            number_of_days (int): Number of days to be use for the\n                test data.\n        \n        \"\"\"\n        \n        train_start_date = self.get_first_day_sold(store_nbr)\n        train_end_date = df_train['date'].max() - np.timedelta64(number_of_days, 'D')\n        \n        X_train, y_train = self.prepare_test_inputs(store_nbr, train_start_date, train_end_date)\n        X_train = self.add_information(X_train)\n        \n        val_start_date = train_end_date + np.timedelta64(1, 'D')\n        val_end_date = df_train['date'].max()\n    \n        X_test, y_test = self.prepare_test_inputs(store_nbr, val_start_date, val_end_date)\n        X_test = self.add_information(X_test)\n    \n        return X_train, y_train, X_test, y_test\n    \n    def get_input_for_prediction(self):\n        \"\"\"Gets the inputs from the test dataframe that will\n        be used to make the final prediction. This function\n        also calls the self.add_information method to prepare\n        the dataframe.\n        \n        Returns:\n            Dataframe will all necessary information for\n            prediction.\n        \n        \"\"\"\n        \n        df = df_test.copy()\n        df = df.set_index(['family','date']).sort_index().drop('id', axis = 1)\n        \n        X = df.loc[df['store_nbr'] == self.store_nbr].drop(['store_nbr'], axis = 1).unstack('family')\n        X.columns = X.columns.map(' - '.join).str.strip(' - ')\n        X = X.reset_index()\n        X = self.add_information(X)\n        return X\n    \n    def merge_to_prediction(self, y_pred, y_columns, X_pred):\n        \"\"\"This function returns the prediction in the same\n        format as the sample_submission file.\n        \n        Args:\n            y_pred (np.array): Values predicted.\n            y_columns (list): Column names.\n            X_pred (DataFrame): Dataframe used for prediction.\n        \n        Returns:\n            Section of submission for the given store.\n        \n        \"\"\"\n        \n        prediction = pd.DataFrame(y_pred, columns = y_columns, index = X_pred.index).unstack()\n        prediction = prediction.reset_index().set_index('date').rename(columns = {0: \"sales\", \"level_0\":\"family\"})\n        prediction['store_nbr'] = self.store_nbr\n        submission = df_test.copy()\n        current_submission = submission.merge(prediction.reset_index(), how = 'inner')\n        \n        return current_submission\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:26:16.319361Z","iopub.execute_input":"2022-04-28T07:26:16.319615Z","iopub.status.idle":"2022-04-28T07:26:16.349573Z","shell.execute_reply.started":"2022-04-28T07:26:16.319579Z","shell.execute_reply":"2022-04-28T07:26:16.348847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Individual Store models class","metadata":{}},{"cell_type":"code","source":"class Store(AuxiliaryFunctions):\n    \"\"\"Class for handling the training and prediction of a\n    XGBRegressor and RandomForestRegressor models. Both\n    model predictions are averaged out before submitting\n    the results.\n    \n    \"\"\"\n    \n    def __init__(self, store_nbr):\n        \"\"\"Setting up all necessary attributes for training\n        and prediction.\n        \n        Args:\n            store_nbr (int): Unique id of the store.\n            \n        Attributes:\n            store_nbr (int): Unique id of the store.\n            random_state (int): Set seed for the random\n                number generator.\n            X_train, y_train (DataFrame): Dataframes for\n                training the model.\n            X_test, y_test (DataFrame): Dataframes for\n                testing the model.\n            X, y (DataFrame): Dataframes that are a\n                combination of training and testing data.\n            xgb_model: XGBoost regressor model.\n            random_forest_model: Random Forest regressor\n                model.\n        \n        \"\"\"\n        \n        self.store_nbr = store_nbr\n        \n        self.random_state = 42\n        \n        self.X_train, self.y_train, self.X_test, self.y_test = self.train_test_split(self.store_nbr)\n        \n        self.X, self.y, _, _ = self.train_test_split(self.store_nbr, 0)\n        \n        self.xgb_model = MultiOutputRegressor(XGBRegressor(booster = \"gbtree\", random_state = self.random_state))\n        self.random_forest_model = RandomForestRegressor(n_estimators = 150, random_state = self.random_state)\n        \n    def train_models(self):\n        \"\"\"Trains both models with the training data.\n        \n        \"\"\"\n        \n        self.xgb_model.fit(self.X_train, self.y_train)\n        self.random_forest_model.fit(self.X_train, self.y_train)\n    \n    def evaluate_models(self):\n        \"\"\"Uses the testing data to evaluate each model, as\n        well as the combination of the two.\n        \n        Returns:\n            evaluation_xgb (float): mean squared log error\n                of the XGBoost regressor model.\n            evaluation_forest (float): mean squared log error\n                of the Random Forest regressor model.\n            evaluation_average (float): mean squared log error\n                of the average prediction of the two models.\n        \n        \"\"\"\n        \n        y_pred_xgb = self.xgb_model.predict(self.X_test).clip(0.0)\n        y_pred_forest = self.random_forest_model.predict(self.X_test).clip(0.0)\n        \n        y_pred_average = (y_pred_xgb + y_pred_forest) / 2\n        \n        evaluation_xgb = mean_squared_log_error(y_pred_xgb, self.y_test.to_numpy())\n        evaluation_forest = mean_squared_log_error(y_pred_forest, self.y_test.to_numpy())\n        \n        evaluation_average = mean_squared_log_error(y_pred_average, self.y_test.to_numpy())\n        \n        return evaluation_xgb, evaluation_forest, evaluation_average\n    \n    def train_and_predict(self):\n        \"\"\"Trains both models using all available testing data,\n        and returns a formatted prediction.\n        \n        Returns:\n            A dataframe with the current predictions with the\n            same formating as the sample_submission.csv file.\n        \n        \"\"\"\n        \n        self.xgb_model.fit(self.X, self.y)\n        \n        self.random_forest_model.fit(self.X, self.y)\n        \n        X_pred = self.get_input_for_prediction()\n        \n        y_pred_xgb_final = self.xgb_model.predict(X_pred).clip(0.0)\n        y_pred_random_forrest_final = self.random_forest_model.predict(X_pred).clip(0.0)\n        \n        y_pred = (y_pred_xgb_final + y_pred_random_forrest_final) / 2\n        \n        return self.merge_to_prediction(y_pred, self.y.columns, X_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:26:16.350945Z","iopub.execute_input":"2022-04-28T07:26:16.351288Z","iopub.status.idle":"2022-04-28T07:26:16.364502Z","shell.execute_reply.started":"2022-04-28T07:26:16.351252Z","shell.execute_reply":"2022-04-28T07:26:16.363787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training all models and submitting prediction","metadata":{}},{"cell_type":"code","source":"def make_prediction():\n    \"\"\"Trains a model for each store and saves the prediction\n    results into a csv file.\n    \n    \"\"\"\n    \n    prediction_list = []\n    for store_nbr in stores['store_nbr'].unique():\n        store = Store(store_nbr = store_nbr)\n        prediction_list.append(store.train_and_predict())\n        print(f'finished predicting store {store_nbr}')\n\n    prediction = prediction_list[0]\n    for i in range(len(prediction_list) - 1):\n        prediction = prediction.append(prediction_list[i + 1])\n\n    final_submission = prediction[['id','sales']].sort_values('id').reset_index().drop('index', axis = 1)\n    final_submission.to_csv(\"submission.csv\", index=False)\n    return final_submission","metadata":{"execution":{"iopub.status.busy":"2022-04-28T07:59:10.322464Z","iopub.execute_input":"2022-04-28T07:59:10.3229Z","iopub.status.idle":"2022-04-28T07:59:10.335461Z","shell.execute_reply.started":"2022-04-28T07:59:10.322868Z","shell.execute_reply":"2022-04-28T07:59:10.334561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = make_prediction()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some possible improvements\n* Use grid search to choose better hyperparameters, which also implies setting more hyperparameters for training the XGBoost and Random Forest models. However, this is expected to take a really long time.\n* Explore other types of models for prediction such as Support Vector Machines and Linear Regression, or a combination of them.\n* Create different models for each product family as opposed to each store.\n* Explore more preprocessing options to deal with the available data, such as changing how the model deals with events and holidays.","metadata":{}}]}
